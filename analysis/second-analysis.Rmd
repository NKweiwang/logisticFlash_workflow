---
title: "Direct model with Greedy Algorithm"
author: "wei wang"
date: 2017-01-16
output: html_document
---

```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

```{r knitr-opts-chunk, include=FALSE}
```

**Last updated:** `r Sys.Date()`

**Code version:** `r workflowr::extract_commit(".", 1)$sha1`


## Abstract

This report is for the simple implementation of Logistic flash. There are two versions of inference for  the model, both of which have advantage and drawbacks. More discussions are needed. We assume that our data matrix is $Y_{N \times P}$ where $Y_{ij} = \pm 1$ (we use $\pm 1$ rather than 0 or 1 here for the Boolean data). We first start from $\mathbf{rank-one}$ and known variance model, which makes the derivation clear and easy to understand.

## Model

We discuss on the model with known variance and rank one structure. Our model is

\begin{eqnarray}
\log\frac{P(Y_{ij} = 1|Z_{ij})}{P(Y_{ij} = -1 | Z_{ij})} = Z_{ij} \\
Z_{ij} = l_i f_j\\
f_j \sim  \sum_{m'} \pi_{m'}^f N(f_j; 0, (\sigma_{m'}^f)^2) \\
l_i \sim  \sum_m \pi_m^l N(l_i; 0, (\sigma_m^l)^2)
\end{eqnarray}
Here we allow that $(\sigma_{1}^f)^2 = 0$ and $(\sigma_{1}^l)^2=0$, which means we include the point mass into the prior for each component for $f$ and $l$.

##  Method: Variational Bayes with Adaptive Shrinkage}
The full likelihood is
\begin{eqnarray}
P(Y) = \int \int  P(Y|l,f)P(l)P(f)dfdl
\end{eqnarray}

### Variational Bayes on $l$ and $f$
\begin{eqnarray}
\log P(Y)  & =&  \log \int \int  P(Y|l,f)P(l)P(f)dfdl  \nonumber \\
 & \geq & \int \int q_f(f)q_l(l) \log \frac{ P(Y|l,f) P(l)P(f)}{q_l(l)q_f(f)} dfdl \nonumber \\
 & = & F(q_f,q_l,\Theta)
\end{eqnarray}
We want to maximize $F(q_f,q_l,q_z,\Theta)$ where $\Theta = (\pi^l,\pi^f,\xi)$, $\pi^l = (\pi^l_1,\cdots,\pi^l_{m_l})$, $\pi^f = (\pi^f_1,\cdots,\pi^f_{m'_f})$ and $(\xi_{N\times P})_{(ij)} = \xi_{ij}, i = 1,\cdots, N, j = 1,\cdots, P$. The $\pi^l$ and $\pi^f$ are from the prior of the $l$ and $f$, and we will talk later about where $\xi$ comes from.

We assume that the approximation of $q_f(f)q_l(l)q_z(Z)$ is as following:

\begin{eqnarray}
q_f(f) = \prod_j q_f(f_j) = \prod_j [\sum_{m'}\alpha^f_{jm'} N(\mu^f_{jm'},s^f_{jm'})]\\
q_l(l) = \prod_i q_l(l_i) = \prod_i [\sum_{m}\alpha^l_{im} N(\mu^l_{im},s^l_{im})] 
\end{eqnarray}

we use the lower boud of the this objective function, which is

\begin{eqnarray}
F(q_f,q_l,\Theta) & \geq &\int\int q(l)q(f)\log \frac{P(l)P(f)H(l,f,Y,\xi)}{q(l)q(f)}dldf\\
H(l,f,Y,\xi) &=& exp\{ \sum_{ij}[\frac{Y_{ij}l_if_j -\xi_{ij}}{2} + \log(h(\xi_{ij}))- \tau(\xi_{ij})(l_i^2 f_j^2 - \xi_{ij}^2) ] \} \\
\tau(\xi) &=& \frac{1}{4 \xi} \tanh(\frac{\xi}{2})
\end{eqnarray}

Given $\xi$, we can apply ASH to estimate the $l_k$ and $f_k$

####  ATM on $l$ and $f$
By plugging in the formula of $H(l,f,\xi)$ into the lowerbound, we can obtain a Ash Type Maximization problem. So the update of $l$ and $f$ are following the ATM solution:

\begin{eqnarray}
\mu_{l_i} &=& \frac{\frac{1}{2}\sum_j(Y_{ij}Ef_j)}{2\sum_j(\tau(\xi_{ij})Ef_j^2)}\\
\sigma^2_{l_i} &=& \frac{1}{2\sum_j(\tau(\xi_{ij})Ef_j^2)}\\
\mu_{f_j} &=& \frac{\frac{1}{2}\sum_i(Y_{ij}El_i)}{2\sum_j(\tau(\xi_{ij})El_i^2)}\\
\sigma^2_{f_j} &=& \frac{1}{2\sum_i(\tau(\xi_{ij})El_i^2)}
\end{eqnarray}


####  update for $\xi$

\begin{eqnarray}
\xi_{ij}^2 = l_i^2f_j^2 = Z_{ij}^2
\end{eqnarray}

###  Rank K version: Greedy Algorithm

####  Rank K model

\begin{eqnarray}
\log\frac{P(Y_{ij} = 1|Z_{ij})}{P(Y_{ij} = -1 | Z_{ij})} = Z_{ij} \\
Z_{ij} = \sum_k l_{ik} f_{kj}\\
f_{kj} \sim  \sum_{m'} \pi_{km'}^f N(f_{kj}; 0, (\sigma_{km'}^f)^2) \\
l_{ik} \sim  \sum_m \pi_{km}^l N(l_{ik}; 0, (\sigma_{km}^l)^2)
\end{eqnarray}

\subsubsection{2.2.2 Variational Inference}
we use the lower boud of the this objective function, which is

\begin{eqnarray}
F(q_f,q_l,\Theta) & \geq &\int\int q(l)q(f)\log \frac{P(l)P(f)H(l,f,Y,\xi)}{q(l)q(f)}dldf\\
H(l,f,Y,\xi) &=& exp\{ \sum_{ij}[\frac{Y_{ij}(\sum_k l_{ik} f_{kj}) -\xi_{ij}}{2} + \log(h(\xi_{ij}))- \tau(\xi_{ij})((\sum_k l_{ik} f_{kj})^2 - \xi_{ij}^2) ] \} \\
\tau(\xi) &=& \frac{1}{4 \xi} \tanh(\frac{\xi}{2})
\end{eqnarray}

By plugging in the formula of $H(l,f,\xi)$ into the lowerbound, we can obtain a Ash Type Maximization problem. So the update of $l$ and $f$ are following the ATM solution:

\begin{eqnarray}
\mu_{l_{ik}} &=& \frac{\sum_j[\frac{1}{2} Y_{ij} -2 (\tau(\xi_{ij})\sum_{t \neq k} (El_{it}Ef_{tj})) ]Ef_{kj}}{2\sum_j(\tau(\xi_{ij})Ef_{kj}^2)}\\
\sigma^2_{l_{ik}} &=& \frac{1}{2\sum_j(\tau(\xi_{ij})Ef_{kj}^2)}\\
\mu_{f_{kj}} &=& \frac{\sum_i[\frac{1}{2} Y_{ij} -2 (\tau(\xi_{ij})\sum_{t \neq k} (El_{it}Ef_{tj})) ]El_{ik}}{2\sum_j(\tau(\xi_{ij})El_{ik}^2)}\\
\sigma^2_{f_{kj}} &=& \frac{1}{2\sum_i(\tau(\xi_{ij})El_{ik}^2)}
\end{eqnarray}

\begin{eqnarray}
\xi_{ij}^2 = (\sum_k l_{ik} f_{kj})^2 = Z_{ij}^2
\end{eqnarray}

#### Residual in Greedy Algorithm

The key of greedy algorithm is how to calculate the residual. Comparing the update in rank one case and rank K case. We decide use
\begin{eqnarray}
residual_k &=&  Y_{ij} -4 [\tau(\xi_{ij})\sum_{t \neq k} (El_{it}Ef_{tj})] \\
\xi_{ij}^2 & = & E_{q_l,q_f}(\sum_{t \neq k} l_{it} f_{tj})^2
\end{eqnarray}
which works pretty well.

Why the residual is important? It is because we need initialize the loading and factor for each new adding factor in the greedy algorithm adaptively.

## Future Work: Backfitting Algorithm

Since in the last section, the update can also be applied into Backfitting algorithm, we can easily extend the current version to more precise version.


## Simulation

### rank 5 model

### 3.1.1 big signal
\begin{eqnarray*}
N &=& 100 \\
P &=& 200\\
(\sigma_{m'}^f) &=& (0.1,0.3,0.6,2)\\
(\pi_{m'}^f) &=& (0.3,0.2,0.2,0.3) \\
(\sigma_{m}^l) &=& (0.1,0.5,1,2)\\
(\pi_{m}^l) &=& (0.3,0.2,0.1,0.4) 
\end{eqnarray*}

```{r,cache=TRUE,eval=TRUE}
set.seed(9)
source('../code/GD_Rfuncrtions.R')
Data = datamaker(100,200,
                 c(0.3,0.2,0.1,0.4),c(0.1,0.5,1,2),1,
                 c(0.3,0.2,0.2,0.3),c(0.1,0.3,0.6,2),1,
                 K = 5)
pY_vec = 1/(1+exp(-as.vector(Data$Z)))
Y_vec_01 = sapply(pY_vec,function(x){rbinom(1,1,x)})
Y_vec = 2*( Y_vec_01 - 1/2)
# Y_vec = Y_vec_01
Y = matrix(Y_vec,nrow = 100, ncol = 200)
gflash = flashr::greedy(Y,K = 8)

Ltest = GL_flash(Y,K = 8)
sqrt(mean((Data$L_true %*% t(Data$F_true) - Ltest$l %*% t(Ltest$f))^2)) / sqrt(mean((Data$L_true %*% t(Data$F_true) )^2))
sqrt(mean((Data$L_true %*% t(Data$F_true) - gflash$l %*% t(gflash$f))^2)) / sqrt(mean((Data$L_true %*% t(Data$F_true) )^2))
par(mfrow = c(2,2),mar = c(5,4,4,2) - 1.9)
plot(svd(Y)$d,main = "eigen values")
plot(as.vector(Data$L_true %*% t(Data$F_true)),as.vector(Ltest$l %*% t(Ltest$f)),main = "estimated structure Lflash")
abline(0,1,col = "red")
plot(as.vector(Data$L_true %*% t(Data$F_true)),as.vector(gflash$l %*% t(gflash$f)),main = "estimated structure flash")
abline(0,1,col = "red")
```


### 3.1.2 mild signal

\begin{eqnarray*}
N &=& 100 \\
P &=& 200\\
(\sigma_{m}^l) &=& (0,0.1,0.5,1,2)\\
(\pi_{m}^l) &=& (0.4, 0.18, 0.12, 0.06, 0.24) \\
(\sigma_{m'}^f) &=& (0, 0.1,0.3,0.6,2)\\
(\pi_{m'}^f) &=& (0.4, 0.18, 0.12, 0.12, 0.18) 
\end{eqnarray*}


```{r,cache=TRUE,eval=TRUE}
set.seed(9)
source('../code/GD_Rfuncrtions.R')
Data = datamaker(100,200,
                 c(0.3,0.2,0.1,0.4),c(0.1,0.5,1,2),0.6,
                 c(0.3,0.2,0.2,0.3),c(0.1,0.3,0.6,2),0.6,
                 K = 5)
pY_vec = 1/(1+exp(-as.vector(Data$Z)))
Y_vec_01 = sapply(pY_vec,function(x){rbinom(1,1,x)})
Y_vec = 2*( Y_vec_01 - 1/2)
# Y_vec = Y_vec_01
Y = matrix(Y_vec,nrow = 100, ncol = 200)
gflash = flashr::greedy(Y,K = 8)

Ltest = GL_flash(Y,K = 8)
sqrt(mean((Data$L_true %*% t(Data$F_true) - Ltest$l %*% t(Ltest$f))^2)) / sqrt(mean((Data$L_true %*% t(Data$F_true) )^2))
sqrt(mean((Data$L_true %*% t(Data$F_true) - gflash$l %*% t(gflash$f))^2)) / sqrt(mean((Data$L_true %*% t(Data$F_true) )^2))
par(mfrow = c(2,2),mar = c(5,4,4,2) - 1.9)
plot(svd(Y)$d,main = "eigen values")
plot(as.vector(Data$L_true %*% t(Data$F_true)),as.vector(Ltest$l %*% t(Ltest$f)),main = "estimated structure Lflash")
abline(0,1,col = "red")
plot(as.vector(Data$L_true %*% t(Data$F_true)),as.vector(gflash$l %*% t(gflash$f)),main = "estimated structure flash")
abline(0,1,col = "red")
```

### 3.1.3 small signal due to sparsity 

\begin{eqnarray*}
N &=& 100 \\
P &=& 200\\
(\sigma_{m}^l) &=& (0,0.1,0.5,1,2)\\
(\pi_{m}^l) &=& (0.7, 0.09, 0.06, 0.03, 0.12) \\
(\sigma_{m'}^f) &=& (0, 0.1,0.3,0.6,2)\\
(\pi_{m'}^f) &=& (0.7, 0.09, 0.06, 0.06, 0.09) 
\end{eqnarray*}


```{r,cache=TRUE,eval=TRUE}
set.seed(9)
source('../code/GD_Rfuncrtions.R')
Data = datamaker(100,200,
                 c(0.3,0.2,0.1,0.4),c(0.1,0.5,1,2),0.3,
                 c(0.3,0.2,0.2,0.3),c(0.1,0.3,0.6,2),0.3,
                 K = 5)
pY_vec = 1/(1+exp(-as.vector(Data$Z)))
Y_vec_01 = sapply(pY_vec,function(x){rbinom(1,1,x)})
Y_vec = 2*( Y_vec_01 - 1/2)
# Y_vec = Y_vec_01
Y = matrix(Y_vec,nrow = 100, ncol = 200)
gflash = flashr::greedy(Y,K = 8)

Ltest = GL_flash(Y,K = 8)
sqrt(mean((Data$L_true %*% t(Data$F_true) - Ltest$l %*% t(Ltest$f))^2)) / sqrt(mean((Data$L_true %*% t(Data$F_true) )^2))
sqrt(mean((Data$L_true %*% t(Data$F_true) - gflash$l %*% t(gflash$f))^2)) / sqrt(mean((Data$L_true %*% t(Data$F_true) )^2))
par(mfrow = c(2,2),mar = c(5,4,4,2) - 1.9)
plot(svd(Y)$d,main = "eigen values")
plot(as.vector(Data$L_true %*% t(Data$F_true)),as.vector(Ltest$l %*% t(Ltest$f)),main = "estimated structure Lflash")
abline(0,1,col = "red")
plot(as.vector(Data$L_true %*% t(Data$F_true)),as.vector(gflash$l %*% t(gflash$f)),main = "estimated structure flash")
abline(0,1,col = "red")
```

## Session Information

```{r session-info}
```
